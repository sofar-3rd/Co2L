import logging
import os.path
import time
import sys
import csv

import numpy as np
import torch
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import tensorboard_logger as tb_logger

from util import set_optimizer, set_cls_optimizer, save_model
from util import AverageMeter, generate_months
from util import adjust_learning_rate, warmup_learning_rate
from util import get_model_stats, current_time

from scipy.spatial import KDTree

from networks.mlp import MySupConEnc
from networks.mlp import MyBinaryClassifier
from losses_negative_only import MySupConLoss
from datasets.data import MalwareDataset, load_range_dataset
from datasets.replay_buffer import ReplayBuffer

import argparse
import torch.backends.cudnn as cudnn


def parse_option():
    parser = argparse.ArgumentParser('argument for training')

    parser.add_argument('--enc_dims', type=str, default='512-384-256-128')

    parser.add_argument('--feature_dim', type=int, default='128')

    # dataset
    parser.add_argument('--dataset_name', type=str, default='gen_androzoo_drebin')

    parser.add_argument('--start_month', type=str)

    parser.add_argument('--end_month', type=str)

    parser.add_argument('--cl_start_month', type=str)

    parser.add_argument('--cl_end_month', type=str)

    parser.add_argument('--num_workers', type=int, default=4)

    # replay buffer
    parser.add_argument('--n_sample_per_class', type=int, default=20)

    # training
    parser.add_argument('--batch_size', type=int, default=256)

    parser.add_argument('--epochs', type=int, default=1000,
                        help='number of training epochs')

    parser.add_argument('--start_epoch', type=int, default=None)

    # classifier training
    parser.add_argument('--cls_bsz', type=int, default=256,
                        help='batch_size')

    parser.add_argument('--cls_epochs', type=int, default=100,
                        help='number of training epochs')

    # optimization
    '''encoder'''
    parser.add_argument('--learning_rate', type=float, default=0.05,
                        help='learning rate')
    parser.add_argument('--lr_decay_epochs', type=str, default='700,800,900',
                        help='where to decay lr, can be a list')
    parser.add_argument('--lr_decay_rate', type=float, default=0.1,
                        help='decay rate for learning rate')
    parser.add_argument('--weight_decay', type=float, default=1e-4,
                        help='weight decay')
    parser.add_argument('--momentum', type=float, default=0.9,
                        help='momentum')

    '''classifier'''
    parser.add_argument('--cls_learning_rate', type=float, default=0.1,
                        help='learning rate')
    parser.add_argument('--cls_lr_decay_epochs', type=str, default='60,75,90',
                        help='where to decay lr, can be a list')
    parser.add_argument('--cls_lr_decay_rate', type=float, default=0.2,
                        help='decay rate for learning rate')
    parser.add_argument('--cls_weight_decay', type=float, default=0,
                        help='weight decay')
    parser.add_argument('--cls_momentum', type=float, default=0.9,
                        help='momentum')

    # temperature
    parser.add_argument('--temp', type=float, default=0.07,
                        help='temperature for loss function')

    # uncertainty sample
    parser.add_argument('--temp', type=float, default=0.07,
                        help='temperature for loss function')
    parser.add_argument('--sample_batch_size', type=float, default=0.07,
                        help='不确定性筛选时要对比多少样本')

    # loss
    parser.add_argument('--family_power', type=float, default=0.5,
                        help='temperature for loss function')
    parser.add_argument('--distill_power', type=float, default=1.0,
                        help='temperature for loss function')
    opt = parser.parse_args()

    return opt


def set_replay_samples(model: MySupConEnc,
                       train_dataset: MalwareDataset,
                       replay_buffer: ReplayBuffer,
                       n_sample_per_class=5):
    """
    计算每个 family 的 embedding 均值，并选出距离中心最近的前 n_sample_per_class 个样本
    """
    mean_embedding_dict = {}
    family_indices = {}

    drebin_inputs = train_dataset.drebin_inputs
    families = train_dataset.families
    labels = train_dataset.labels

    model.eval()
    with torch.no_grad():
        embeddings = model.encoder(drebin_inputs)

    for idx, family in enumerate(families):
        if family not in mean_embedding_dict:
            mean_embedding_dict[family] = {'total': torch.zeros_like(embeddings[0]), 'cnt': 0}
            family_indices[family] = []
        mean_embedding_dict[family]['total'] += embeddings[idx]
        mean_embedding_dict[family]['cnt'] += 1
        family_indices[family].append(idx)

    # 计算样本中心距离
    for f in mean_embedding_dict:
        mean_embedding_dict[f]['mean'] = mean_embedding_dict[f]['total'] / mean_embedding_dict[f]['cnt']

    # 计算距离并选出前 5 个样本
    top5_indices = {}
    for f, indices in family_indices.items():
        center = mean_embedding_dict[f]['mean']  # 获取该 family 的中心
        distances = torch.tensor(
            [torch.norm(embeddings[idx] - center, 2).item() for idx in indices]
        )  # 计算距离
        sorted_indices = torch.argsort(distances)[:n_sample_per_class]  # 选出前 5 个最近的样本索引
        top5_indices[family] = [indices[i] for i in sorted_indices]  # 转换为原始数据索引

    for f, indices in top5_indices:
        for index in indices:
            replay_buffer.update(drebin_inputs[index], labels[index], families[index])


def set_loader(opt, dataset_name, start_month, end_month):
    features, labels, families = load_range_dataset(start_month, end_month, dataset_name)
    # original_dim = features.shape[1]

    train_dataset = MalwareDataset(features, labels, families)
    train_loader = DataLoader(
        train_dataset, batch_size=opt.batch_size, shuffle=True,
        num_workers=opt.num_workers, pin_memory=True, sampler=None)

    cls_train_loader = DataLoader(
        train_dataset, batch_size=opt.cls_bsz, shuffle=True,
        num_workers=opt.num_workers, pin_memory=True, sampler=None)

    return train_dataset, train_loader, cls_train_loader


def set_model(opt):
    enc_dims = opt.enc_dims.split('-')
    model = MySupConEnc(enc_dims)
    criterion = MySupConLoss(temperature=opt.temp)

    if torch.cuda.is_available():
        model = model.cuda()
        criterion = criterion.cuda()
        cudnn.benchmark = True

    return model, criterion


def set_classifier(opt):
    criterion = torch.nn.CrossEntropyLoss()
    classifier = MyBinaryClassifier(feature_dim=opt.feature_dim)

    return classifier, criterion


"""
两种训练模式
初始训练：
1、读取初始训练集
2、训练模型
3、存储回放样本
4、保存模型和样本
持续训练：
1、读取验证集
2、根据不确定性分析挑选样本
3、使用新样本与缓冲区样本微调模型
4、更新缓冲区样本
"""


def train(train_loader, model: MySupConEnc, model2: MySupConEnc, criterion, optimizer, epoch, opt):
    model.train()

    batch_time = AverageMeter()
    data_time = AverageMeter()
    losses = AverageMeter()

    end = time.time()

    for idx, (drebin_input, labels, families) in enumerate(train_loader):
        data_time.update(time.time() - end)

        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)

        features, encoded = model(drebin_input, return_feat=True)

        # IRD (current)
        if opt.target_task > 0:
            features1_prev_task = features

            features1_sim = torch.div(torch.matmul(features1_prev_task, features1_prev_task.T), opt.current_temp)
            logits_mask = torch.scatter(
                torch.ones_like(features1_sim),
                1,
                torch.arange(features1_sim.size(0)).view(-1, 1).cuda(non_blocking=True),
                0
            )
            logits_max1, _ = torch.max(features1_sim * logits_mask, dim=1, keepdim=True)
            features1_sim = features1_sim - logits_max1.detach()

            row_size = features1_sim.size(0)
            logits1 = torch.exp(features1_sim[logits_mask.bool()].view(row_size, -1)) / torch.exp(
                features1_sim[logits_mask.bool()].view(row_size, -1)).sum(dim=1, keepdim=True)

        # Asym SupCon
        loss = criterion(features, labels)
        family_loss = criterion(features, families)

        # IRD (past)
        if opt.target_task > 0:
            with torch.no_grad():
                features2_prev_task = model2(drebin_input)
                features2_sim = torch.div(torch.matmul(features2_prev_task, features2_prev_task.T), opt.past_temp)

                logits_max2, _ = torch.max(features2_sim * logits_mask, dim=1, keepdim=True)
                features2_sim = features2_sim - logits_max2.detach()

                logits2 = torch.exp(features2_sim[logits_mask.bool()].view(row_size, -1)) / torch.exp(
                    features2_sim[logits_mask.bool()].view(row_size, -1)).sum(dim=1, keepdim=True)

            loss_distill = (-logits2 * torch.log(logits1)).sum(1).mean()
            loss += opt.family_power * family_loss + opt.distill_power * loss_distill

        bsz = labels.shape[0]
        losses.update(loss.item(), bsz)

        # SGD
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # measure elapsed time
        batch_time.update(time.time() - end)
        end = time.time()

        # print info
        print('Train: [{0}][{1}/{2}]\t'
              'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
              'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
              'loss {loss.val:.3f} ({loss.avg:.3f})'.format(
            epoch, idx + 1, len(train_loader), batch_time=batch_time,
            data_time=data_time, loss=losses))
        sys.stdout.flush()

    return losses.avg, model2


# param：训练加载器、编码器、分类器、损失函数、优化器、训练周期、writer、opt
def classifier_train(train_loader, model: MySupConEnc, classifier: MyBinaryClassifier, criterion, optimizer, epoch, opt):
    """one epoch training"""
    model.eval()
    classifier.train()

    batch_time = AverageMeter()
    data_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()

    end = time.time()
    acc = 0.0
    cnt = 0.0

    for idx, (drebin_inputs, labels, families) in enumerate(train_loader):
        data_time.update(time.time() - end)

        drebin_inputs = drebin_inputs.cuda(non_blocking=True)
        labels = labels.cuda(non_blocking=True)
        bsz = labels.shape[0]

        # warm-up learning rate
        warmup_learning_rate(opt, epoch, idx, len(train_loader), optimizer)

        # compute loss
        with torch.no_grad():
            features = model.encoder(drebin_inputs)
        output = classifier(features.detach())
        loss = criterion(output, labels)

        # update metric
        losses.update(loss.item, bsz)
        acc += (output.argmax(1) == labels).float().sum().item()
        cnt += bsz

        # SGD
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # measure elapsed time
        batch_time.update(time.time() - end)
        end = time.time()

        # print info
        print('Train: [{0}][{1}/{2}]\t'
              'BT {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
              'DT {data_time.val:.3f} ({data_time.avg:.3f})\t'
              'loss {loss.val:.3f} ({loss.avg:.3f})\t'
              'Acc@1 {top1:.3f}'.format(
            epoch, idx + 1, len(train_loader), batch_time=batch_time,
            data_time=data_time, loss=losses, top1=acc / cnt * 100.))
        sys.stdout.flush()

    return losses.avg, acc / cnt * 100.


def validate(drebin_inputs, labels, model: MySupConEnc, classifier: MyBinaryClassifier, cur_month_str, opt):
    model.eval()
    classifier.eval()

    n_cls = 2

    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()

    corr = torch.zeros(n_cls)
    cnt = torch.zeros(n_cls)

    with torch.no_grad():
        drebin_inputs = drebin_inputs.float().cuda()
        labels = labels.cuda()
        bsz = labels.shape[0]

        # forward
        embeddings = model.encoder(drebin_inputs)
        logits = classifier(embeddings)
        pred_labels = logits.argmax(dim=1)

        tpr, tnr, fpr, fnr, acc, precision, f1_score = get_model_stats(labels, pred_labels)
        # write to csv
        metric_row = [cur_month_str] + [round(x, 4) for x in (tpr, tnr, fpr, fnr, acc, precision, f1_score)]
        opt.metric_csv.writerow(metric_row)

    # 返回低维特征嵌入、预测标签用于不确定性筛选
    return embeddings, pred_labels


def uncertainly_sample(model: MySupConEnc, sample_cnt, opt, year_month,
                       train_drebin, train_labels, train_families,
                       test_drebin, test_labels, test_families,
                       test_pred_labels=None, train_embeddings=None, test_embeddings=None):
    logging.info('Begin select sample update replay buffer')
    train_drebin_tensor = torch.from_numpy(train_drebin).float().cuda()
    if train_embeddings is None:
        with torch.no_grad():
            train_embeddings = model.encoder(train_drebin_tensor)
    train_embeddings_n = torch.nn.functional.normalize(train_embeddings)
    train_embeddings_n = train_embeddings_n.cpu().detach().numpy()

    test_drebin_tensor = torch.from_numpy(test_drebin).float().cuda()
    if test_embeddings is None:
        with torch.no_grad():
            test_embeddings = model.encoder(test_drebin_tensor)
    test_embeddings_n = torch.nn.functional.normalize(test_embeddings)
    test_embeddings_n = test_embeddings_n.cpu().detach().numpy()

    sample_indices = []
    sample_scores = []

    logging.info('Building KDTree')
    tree = KDTree(train_embeddings_n)
    logging.info('Querying KDTree')
    all_distances, all_indices = tree.query(test_embeddings_n, k=train_embeddings_n.shape[0], workers=4)
    logging.info('Finished querying KDTree')

    # each batch is to get one loss for one test sample
    batch_time = AverageMeter()
    data_time = AverageMeter()
    end = time.time()

    bsize = opt.sample_batch_size
    sample_total = test_embeddings.shape[0]

    # record file
    csv_header = ['loss_scores', 'pred_label', 'true_label', 'family', 'FN']
    select_sample_file = open(f"{opt.select_per_month}_{year_month}", 'w', encoding='utf-8', newline='')
    selected_csv = csv.writer(select_sample_file)
    selected_csv.writerow(csv_header)

    for i in range(sample_total):
        # bsize-1 nearest neighbors of the test sample i
        batch_indices = all_indices[i][:bsize - 1]

        # sample embedding
        sample_embedding = test_embeddings[i]

        # x_batch
        train_embeddings_batch = train_embeddings[batch_indices]
        embeddings_batch = torch.cat((sample_embedding, train_embeddings_batch), dim=0)

        # sample pred label
        sample_label = test_pred_labels[i]

        # y_batch
        train_labels_batch = train_labels[batch_indices]
        labels_batch = np.hstack((sample_label, train_labels_batch))
        labels_batch = torch.from_numpy(labels_batch).cuda()

        data_time.update(time.time() - end)

        contrast_loss = MySupConLoss(contrast_mode='all', base_temperature=1, temperature=1).cuda()
        with torch.no_grad():
            loss = contrast_loss.forward(embeddings_batch, labels_batch, reduction='keep_dim')

        # measure elapsed time
        batch_time.update(time.time() - end)
        end = time.time()

        sample_scores.append(loss[0])
        if (i + 1) % 10 == 0:
            logging.debug('Train + Test: [0][{0}/{1}]\t'
                          'BT {batch_time.val:.3f} ({batch_time.avg:.3f})  '
                          'DT {data_time.val:.3f} ({data_time.avg:.3f})  '
                          'i {i} loss {l}'.format(i + 1, sample_total, batch_time=batch_time,
                                                  data_time=data_time, i=i, l=loss[0]))

    sorted_sample_scores = list(sorted(list(enumerate(sample_scores)), key=lambda item: item[1], reverse=True))
    logging.info(f'sort_sample_scores[:100]: {sorted_sample_scores[:100]}')
    sample_cnt = 0
    for idx, score in sorted_sample_scores:
        sample_indices.append(idx)
        selected_csv.writerow([score,
                               test_pred_labels[idx],
                               test_labels[idx],
                               test_families[idx],
                               test_pred_labels[idx] == test_labels[idx]])

    return sample_indices, sample_scores


def main():
    opt = parse_option()

    # build model and criterion
    model, criterion = set_model(opt)
    classifier, cls_criterion = set_classifier(opt)
    model2, _ = set_model(opt)
    model2.eval()

    # build optimizer
    optimizer = set_optimizer(opt, model)
    optimizer_cls = set_cls_optimizer(opt, classifier)

    # tensorboard
    writer = SummaryWriter(log_dir=opt.tb_folder)
    opt.writer = writer

    # calculate cl months
    months_list = generate_months(opt.cl_start_month, opt.cl_end_month)

    # experiment result record
    res_csv_header = ['date', 'TPR', 'TNR', 'FPR', 'FNR', 'ACC', 'PREC', 'F1']
    metric_file = open(opt.result, 'w', encoding='utf-8', newline='')
    metric_csv = csv.writer(metric_file)
    metric_csv.writerow(res_csv_header)
    opt.metric_csv = metric_csv

    """
    initial train
    """
    opt.initial_train = True
    print('Initial train without replay buffer')
    train_dataset, train_loader, cls_train_loader = set_loader(opt, opt.dataset_name, opt.start_month, opt.end_month)
    epochs = opt.start_epoch if opt.start_epoch is not None else opt.epochs

    for epoch in range(1, epochs + 1):
        adjust_learning_rate(opt, optimizer, epoch)

        # train for on epoch
        time1 = time.time()
        loss, model2 = train(train_loader, model, model2, criterion, optimizer, epoch, opt)
        time2 = time.time()
        print('epoch {}, total time {:.2f}'.format(epoch, time2 - time1))

        # tensorboard writer
        writer.add_scalar(tag='loss/initial_train', scalar_value=loss, global_step=epoch)
        writer.add_scalar(tag='lr/initial_train', scalar_value=optimizer.param_groups[0]['lr'], global_step=epoch)

    # classifier training
    cls_epochs = opt.cls_epochs
    for cls_epoch in range(1, cls_epochs + 1):
        time_b = time.time()
        loss, acc = classifier_train(cls_train_loader, model, classifier, cls_criterion, optimizer_cls, cls_epoch, opt)
        time_e = time.time()
        print(f'epoch {cls_epoch}, total time {time_e-time_b:.2f}')

        # tensorboard writer
        writer.add_scalar(tag='loss/initial_train_cls', scalar_value=loss, global_step=cls_epoch)
        writer.add_scalar(tag='acc/initial_train_cls', scalar_value=acc, global_step=cls_epoch)

    # select sample for replay buffer
    replay_buffer = ReplayBuffer(n_sample_per_class=opt.n_sample_per_class)
    set_replay_samples(model, train_dataset, replay_buffer)

    # save model
    ctime = current_time()
    model_path = os.path.join(os.getcwd(), 'experiment', 'model', ctime)
    os.makedirs(model_path, exist_ok=True)
    save_file = os.path.join(model_path, '{}to{}.pth'.format(opt.start_month, opt.end_month))
    save_model(model, optimizer, opt, opt.epochs, save_file)

    # continual learning
    for month in months_list:
        # load data
        drebin_inputs, labels, families = load_range_dataset(month, month)
        embeddings, pred_labels = validate(drebin_inputs=drebin_inputs, labels=labels,
                                           model=model, classifier=classifier,
                                           cur_month_str=month, opt=opt)

        uncertainly_sample()


if __name__ == '__main__':
    main()
